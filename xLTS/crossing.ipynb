{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded roads_gdf with CRS: EPSG:6491\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# Load the necessary libraries\n",
    "##############################\n",
    "\n",
    "import os\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rtree\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.ops import unary_union\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Set Directory Paths and File Name Strings\n",
    "###########################################\n",
    "\n",
    "base_dir = r\"C:\\Users\\natda\\Desktop\\NatDave\\Academics\\PhD_NU\\RESEARCH\\Traffic_Stress\\Boston\"\n",
    "\n",
    "roads_filename = \"street_network.shp\"\n",
    "nodes_filename = \"nodes.shp\"\n",
    "merge_or_not_nodes_filename = \"merge_or_not_nodes.shp\"\n",
    "junctions_filename = \"junctions.shp\"\n",
    "crossings_filename = \"crossings.shp\"\n",
    "\n",
    "roads_path = os.path.join(base_dir, roads_filename)\n",
    "nodes_path = os.path.join(base_dir, nodes_filename)\n",
    "merge_or_not_nodes_path = os.path.join(base_dir, merge_or_not_nodes_filename)\n",
    "junctions_path = os.path.join(base_dir, junctions_filename)\n",
    "crossings_path = os.path.join(base_dir, crossings_filename)\n",
    "\n",
    "############################\n",
    "# Load the Road Network Data\n",
    "############################\n",
    "\n",
    "roads_gdf = gpd.read_file(roads_path)\n",
    "if 'unique_id' in roads_gdf.columns and roads_gdf.index.name != 'unique_id':\n",
    "    roads_gdf = roads_gdf.set_index('unique_id', drop=False)\n",
    "\n",
    "print(\"Loaded roads_gdf with CRS:\", roads_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 13096 nodes in total.\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Create Nodes\n",
    "##############\n",
    "\n",
    "# Filter and create a dict for valid road segments\n",
    "roads_dict = {}\n",
    "for idx, row in roads_gdf.iterrows():\n",
    "    qExclude = row.get('qExclude', 0)\n",
    "    qNoAccess = row.get('qNoAccess', 0)\n",
    "    geom = row.geometry\n",
    "\n",
    "    # Only include valid segments in roads_dict\n",
    "    if qExclude not in {1, 5} and qNoAccess != 1 and geom is not None:\n",
    "        roads_dict[idx] = {\n",
    "            'geometry': geom,\n",
    "            'StOperNEU': row.get('StOperNEU', 0),\n",
    "            'STREETNAME': row.get('STREETNAME', \"\")\n",
    "        }\n",
    "\n",
    "# Collect start and end points of valid road segments\n",
    "endpoints_dict = defaultdict(list)\n",
    "for sid, data in roads_dict.items():\n",
    "    geom = data['geometry']\n",
    "\n",
    "    # Handle geometry to get the very first and very last coordinates\n",
    "    if geom.geom_type == \"LineString\":\n",
    "        first_point = geom.coords[0]\n",
    "        last_point = geom.coords[-1]\n",
    "    elif geom.geom_type == \"MultiLineString\":\n",
    "        first_point = geom.geoms[0].coords[0]     # Start of the first LineString\n",
    "        last_point = geom.geoms[-1].coords[-1]    # End of the last LineString\n",
    "    else:\n",
    "        continue                                  # Skip unsupported geometries\n",
    "\n",
    "    # Record the start and end points\n",
    "    endpoints_dict[first_point].append(sid)\n",
    "    endpoints_dict[last_point].append(sid)\n",
    "\n",
    "# Identify potential intersections\n",
    "nodes_list = []\n",
    "node_id = 1\n",
    "\n",
    "for pt, seg_ids in endpoints_dict.items():\n",
    "    if len(seg_ids) >= 3:\n",
    "        nodes_list.append({\n",
    "            'NODE_ID': node_id,\n",
    "            'INC_LINKS': seg_ids,\n",
    "            'NUM_LINKS': len(seg_ids),\n",
    "            'geometry': Point(pt)\n",
    "        })\n",
    "        node_id += 1\n",
    "\n",
    "nodes_gdf = gpd.GeoDataFrame(nodes_list, crs=roads_gdf.crs)\n",
    "nodes_gdf.to_file(nodes_path)\n",
    "print(f\"Saved {len(nodes_gdf)} nodes in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559 out of 13096 nodes are path-crossing nodes.\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# Identify Path-Crossing Nodes\n",
    "##############################\n",
    "\n",
    "# Identify links with \"Off-Road Path\"\n",
    "off_road_links = set(roads_gdf.loc[roads_gdf['bike_type'] == \"Off-Road Path\", 'unique_id'])\n",
    "\n",
    "# Check for path crossing nodes\n",
    "def is_path_crossing(inc_links):\n",
    "    # Count how many incident links are \"Off-Road Path\"\n",
    "    off_road_count = sum(1 for link in inc_links if link in off_road_links)\n",
    "    return off_road_count >= 2  # At least two off-road paths\n",
    "\n",
    "# Apply the function and create a new column\n",
    "nodes_gdf['PATH_XING'] = nodes_gdf['INC_LINKS'].apply(is_path_crossing)\n",
    "\n",
    "# Count path-crossing and total nodes\n",
    "num_path_crossing = nodes_gdf['PATH_XING'].sum()\n",
    "total_nodes = len(nodes_gdf)\n",
    "\n",
    "print(f\"{num_path_crossing} out of {total_nodes} nodes are path-crossing nodes.\")\n",
    "\n",
    "# Overwrite the nodes_gdf shapefile\n",
    "nodes_gdf.to_file(nodes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 unique nodes to inspect (merge or not).\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Build Spatial Index for Junctions\n",
    "###################################\n",
    "spatial_index = rtree.index.Index(\n",
    "    (idx, node['geometry'].bounds, idx) for idx, node in enumerate(nodes_list)\n",
    ")\n",
    "\n",
    "def get_node(endpoint):\n",
    "    \"\"\"Find the exact node matching the endpoint.\"\"\"\n",
    "    candidates = [nodes_list[idx] for idx in spatial_index.intersection(Point(endpoint).bounds)]\n",
    "    return next((n for n in candidates if n['geometry'].equals(Point(endpoint))), None)\n",
    "\n",
    "def has_another_stoperneu_11(node, shared_segments):\n",
    "    \"\"\"Check if the node has another link on a divided highway.\"\"\"\n",
    "    return any(\n",
    "        roads_dict[sid]['StOperNEU'] == 11 and sid not in shared_segments for sid in node['INC_LINKS']\n",
    "    )\n",
    "\n",
    "##########################################\n",
    "# Flag Nodes for Inspection Using Indexing\n",
    "##########################################\n",
    "\n",
    "merge_or_not_nodes = []\n",
    "\n",
    "for sid, road_data in roads_dict.items():\n",
    "    if road_data['StOperNEU'] != 11:\n",
    "        continue\n",
    "\n",
    "    # Get endpoints\n",
    "    geom = road_data['geometry']\n",
    "    endpoints = (\n",
    "        [geom.coords[0], geom.coords[-1]] if geom.geom_type == \"LineString\" else \n",
    "        [geom.geoms[0].coords[0], geom.geoms[-1].coords[-1]] if geom.geom_type == \"MultiLineString\" else\n",
    "        None\n",
    "    )\n",
    "    if endpoints is None:\n",
    "        continue\n",
    "\n",
    "    # Find nodes\n",
    "    node_a, node_b = get_node(endpoints[0]), get_node(endpoints[1])\n",
    "    if not node_a or not node_b:\n",
    "        continue\n",
    "\n",
    "    # Check spatial and link conditions\n",
    "    distance = node_a['geometry'].distance(node_b['geometry'])\n",
    "    if not (14 <= distance <= 28):\n",
    "        continue\n",
    "\n",
    "    num_links_a, num_links_b = len(node_a['INC_LINKS']), len(node_b['INC_LINKS'])\n",
    "    if not ((num_links_a == 3 and num_links_b == 4) or (num_links_a == 4 and num_links_b == 3)):\n",
    "        continue\n",
    "\n",
    "    # Check for individual divided road segments\n",
    "    shared_segments = set(node_a['INC_LINKS']) & set(node_b['INC_LINKS'])\n",
    "    if has_another_stoperneu_11(node_a, shared_segments) and has_another_stoperneu_11(node_b, shared_segments):\n",
    "        # Add to inspection list\n",
    "        merge_or_not_nodes.extend([\n",
    "            {'NODE_ID': node_a['NODE_ID'], 'geometry': node_a['geometry']},\n",
    "            {'NODE_ID': node_b['NODE_ID'], 'geometry': node_b['geometry']}\n",
    "        ])\n",
    "\n",
    "# Create GeoDataFrame and drop duplicates\n",
    "merge_or_not_nodes_gdf = gpd.GeoDataFrame(merge_or_not_nodes, crs=roads_gdf.crs)\n",
    "merge_or_not_nodes_gdf = merge_or_not_nodes_gdf.drop_duplicates(subset=['geometry'])\n",
    "\n",
    "merge_or_not_nodes_gdf.to_file(merge_or_not_nodes_path)\n",
    "\n",
    "print(f\"There are {len(merge_or_not_nodes_gdf)} unique nodes to inspect (merge or not).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersections saved: 12352 total.\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# Merge Nodes\n",
    "#############\n",
    "\n",
    "def create_proper_junctions(nodes, roads, threshold):\n",
    "    \"\"\"\n",
    "    Merge nodes within 'threshold' distance if they meet the conditions.\n",
    "    Path-crossing nodes only merge with other path-crossing nodes,\n",
    "    and non-path-crossing nodes only merge with other non-path-crossing nodes.\n",
    "    \"\"\"\n",
    "    merged_nodes = []\n",
    "    standalone_nodes = []\n",
    "    processed = set()\n",
    "\n",
    "    for node in nodes.itertuples():\n",
    "        if node.Index in processed:\n",
    "            continue\n",
    "\n",
    "        # Find nearby nodes within the threshold distance\n",
    "        close_nodes = nodes[nodes.geometry.distance(node.geometry) <= threshold]\n",
    "        inc_links = set(node.INC_LINKS)\n",
    "        constituent_nodes = {node.NODE_ID}  # Use a set to ensure unique node IDs\n",
    "\n",
    "        # Track connecting links to exclude\n",
    "        links_to_exclude = set()\n",
    "\n",
    "        for cn in close_nodes.itertuples():\n",
    "            # Skip already processed nodes\n",
    "            if cn.Index in processed:\n",
    "                continue\n",
    "\n",
    "            # Ensure that only similar types of nodes are merged\n",
    "            if cn.PATH_XING != node.PATH_XING:\n",
    "                continue  # Do not merge different types of nodes (path vs. non-path)\n",
    "\n",
    "            # Both nodes must have at least one segment with StOperNEU = 11\n",
    "            has_highway_link_current = any(roads[sid]['StOperNEU'] == 11 for sid in node.INC_LINKS)\n",
    "            has_highway_link_nearby = any(roads[sid]['StOperNEU'] == 11 for sid in cn.INC_LINKS)\n",
    "\n",
    "            if not (has_highway_link_current and has_highway_link_nearby):\n",
    "                continue  # Skip if either node does not meet the divided highway condition\n",
    "\n",
    "            # Identify and exclude shared links or connecting links\n",
    "            shared_links = set(node.INC_LINKS) & set(cn.INC_LINKS)\n",
    "\n",
    "            # Detect connecting links by checking if their geometry connects the two nodes\n",
    "            for link in shared_links:\n",
    "                link_geom = roads[link]['geometry']\n",
    "                if (\n",
    "                    link_geom.geom_type == \"LineString\"\n",
    "                    and (\n",
    "                        link_geom.coords[0] == (node.geometry.x, node.geometry.y)\n",
    "                        and link_geom.coords[-1] == (cn.geometry.x, cn.geometry.y)\n",
    "                        or link_geom.coords[0] == (cn.geometry.x, cn.geometry.y)\n",
    "                        and link_geom.coords[-1] == (node.geometry.x, node.geometry.y)\n",
    "                    )\n",
    "                ):\n",
    "                    links_to_exclude.add(link)\n",
    "\n",
    "            # Add the node to the merging group and exclude shared or connecting links\n",
    "            constituent_nodes.add(cn.NODE_ID)\n",
    "            inc_links.update(sid for sid in cn.INC_LINKS if sid not in links_to_exclude)\n",
    "\n",
    "        # Finalize merging by ensuring unique links and nodes\n",
    "        final_inc_links = list(set(inc_links) - links_to_exclude)\n",
    "        final_constituent_nodes = list(constituent_nodes)\n",
    "\n",
    "        # Check if merging will result in at least three unique links\n",
    "        if len(final_constituent_nodes) > 1 and len(final_inc_links) >= 3:\n",
    "            mg = unary_union([n.geometry for n in nodes.loc[nodes['NODE_ID'].isin(final_constituent_nodes)].itertuples()])\n",
    "            merged_nodes.append({\n",
    "                'INTER_ID': node.NODE_ID,\n",
    "                'INC_LINKS': final_inc_links,  # Ensure unique links are stored\n",
    "                'NUM_LINKS': len(final_inc_links),\n",
    "                'geometry': mg.centroid,\n",
    "                'WAS_MERGED': True,\n",
    "                'CON_NODES': final_constituent_nodes  # Ensure unique node IDs are stored\n",
    "            })\n",
    "            processed.update(nodes.loc[nodes['NODE_ID'].isin(final_constituent_nodes)].index)\n",
    "        else:\n",
    "            # Keep the node as standalone if merging is invalid\n",
    "            standalone_nodes.append({\n",
    "                'INTER_ID': node.NODE_ID,\n",
    "                'INC_LINKS': list(set(node.INC_LINKS)),  # Ensure unique incident links\n",
    "                'NUM_LINKS': len(set(node.INC_LINKS)),\n",
    "                'geometry': node.geometry,\n",
    "                'WAS_MERGED': False,\n",
    "                'CON_NODES': [node.NODE_ID]\n",
    "            })\n",
    "            processed.add(node.Index)\n",
    "\n",
    "    mgdf = gpd.GeoDataFrame(merged_nodes, geometry='geometry', crs=nodes.crs)\n",
    "    sgdf = gpd.GeoDataFrame(standalone_nodes, geometry='geometry', crs=nodes.crs)\n",
    "    return pd.concat([mgdf, sgdf], ignore_index=True)\n",
    "\n",
    "\n",
    "junctions_gdf = create_proper_junctions(nodes_gdf, roads_dict, threshold=28)\n",
    "junctions_gdf.to_file(junctions_path)\n",
    "print(f\"Intersections saved: {len(junctions_gdf)} total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legs_dict created successfully.\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Create Legs Dictionary\n",
    "########################\n",
    "\n",
    "def calc_bearing(jxy, geom):\n",
    "    \"\"\"Compute bearing (0 to 360°, north=0) from intersection jxy outward.\"\"\"\n",
    "    if geom.geom_type == \"LineString\":\n",
    "        coords = geom.coords\n",
    "    elif geom.geom_type == \"MultiLineString\" and len(geom.geoms) > 0:\n",
    "        coords = geom.geoms[0].coords\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        return None\n",
    "\n",
    "    sx, sy = coords[0]\n",
    "    ex, ey = coords[-1]\n",
    "\n",
    "    ds = (sx - jxy[0])**2 + (sy - jxy[1])**2\n",
    "    de = (ex - jxy[0])**2 + (ey - jxy[1])**2\n",
    "\n",
    "    if ds < de:\n",
    "        dx, dy = ex - jxy[0], ey - jxy[1]\n",
    "    else:\n",
    "        dx, dy = sx - jxy[0], sy - jxy[1]\n",
    "\n",
    "    angle_rad = math.atan2(dx, dy)\n",
    "    deg = math.degrees(angle_rad)\n",
    "    return deg + 360 if deg < 0 else deg\n",
    "\n",
    "def point_from_bearing(jxy, bearing_deg, dist_m=6):\n",
    "    \"\"\"Compute a point dist_m away from jxy at bearing_deg (0=North, clockwise).\"\"\"\n",
    "    theta = math.radians(bearing_deg)\n",
    "    return (\n",
    "        jxy[0] + dist_m * math.sin(theta),\n",
    "        jxy[1] + dist_m * math.cos(theta)\n",
    "    )\n",
    "\n",
    "legs_dict = {}\n",
    "for _, row in junctions_gdf.iterrows():\n",
    "    j_id = row['INTER_ID']\n",
    "    jxy = (row.geometry.x, row.geometry.y)\n",
    "    INC_LINKS = row['INC_LINKS']\n",
    "\n",
    "    # 1) Build raw_legs\n",
    "    raw_legs = []\n",
    "    for sid in INC_LINKS:\n",
    "        rd = roads_dict[sid]\n",
    "        geom    = rd['geometry']\n",
    "        st_name = rd['STREETNAME']\n",
    "        divided = (rd['StOperNEU'] == 11)\n",
    "\n",
    "        brg = calc_bearing(jxy, geom)\n",
    "        if brg is not None:\n",
    "            raw_legs.append({\n",
    "                'LINKS': [sid],\n",
    "                'ST_NAME': st_name,\n",
    "                'AVG_BRG': brg,\n",
    "                'DIVIDED': divided\n",
    "            })\n",
    "\n",
    "    # 2) Sort by bearing if not empty\n",
    "    df = pd.DataFrame(raw_legs)\n",
    "    if not df.empty and 'AVG_BRG' in df.columns:\n",
    "        df = df.sort_values('AVG_BRG', ascending=False).reset_index(drop=True)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['LINKS','ST_NAME','AVG_BRG','DIVIDED'])\n",
    "\n",
    "    # 3) Merge adjacent divided links if n>3\n",
    "    combined_legs = []\n",
    "    used = set()\n",
    "    n = len(df)\n",
    "\n",
    "    if n > 3:\n",
    "        for i in range(n):\n",
    "            if i in used:\n",
    "                continue\n",
    "            leg_i = df.iloc[i].to_dict()\n",
    "            j = (i + 1) % n\n",
    "            if j not in used and n > 1:\n",
    "                leg_j = df.iloc[j].to_dict()\n",
    "                if leg_i['DIVIDED'] and leg_j['DIVIDED'] and leg_i['ST_NAME'] == leg_j['ST_NAME']:\n",
    "                    combined_legs.append({\n",
    "                        'LINKS': leg_i['LINKS'] + leg_j['LINKS'],\n",
    "                        'ST_NAME': leg_i['ST_NAME'],\n",
    "                        'AVG_BRG': (leg_i['AVG_BRG'] + leg_j['AVG_BRG']) / 2.0,\n",
    "                        'DIVIDED': True\n",
    "                    })\n",
    "                    used.update([i, j])\n",
    "                    continue\n",
    "            combined_legs.append(leg_i)\n",
    "            used.add(i)\n",
    "    else:\n",
    "        combined_legs = [df.iloc[k].to_dict() for k in range(n)]\n",
    "\n",
    "    # 4) Re-sort final & assign rank\n",
    "    if combined_legs:\n",
    "        final_df = pd.DataFrame(combined_legs)\n",
    "        if not final_df.empty and 'AVG_BRG' in final_df.columns:\n",
    "            final_df = final_df.sort_values('AVG_BRG', ascending=False).reset_index(drop=True)\n",
    "            final_df['CC_RANK'] = final_df.index + 1\n",
    "        else:\n",
    "            final_df = pd.DataFrame(columns=['LINKS','ST_NAME','AVG_BRG','DIVIDED','CC_RANK'])\n",
    "    else:\n",
    "        final_df = pd.DataFrame(columns=['LINKS','ST_NAME','AVG_BRG','DIVIDED','CC_RANK'])\n",
    "\n",
    "    legs_dict[j_id] = {\n",
    "        'INTERSECTION_XY': jxy,\n",
    "        'J_NODES': [j_id],\n",
    "        'LEGS_DF': final_df\n",
    "    }\n",
    "\n",
    "print(\"legs_dict created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 40825 crossings to base directory.\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Create Crossing Geometries\n",
    "#############################\n",
    "\n",
    "def create_crossings(legs_dictionary, roads_geo_df, out_path):\n",
    "    cross_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries())\n",
    "    cross_gdf['geometry'] = None\n",
    "    offset = 5\n",
    "    row_idx = 0\n",
    "\n",
    "    for j_id, j_data in legs_dictionary.items():\n",
    "        df = j_data['LEGS_DF']\n",
    "        jxy = j_data['INTERSECTION_XY']\n",
    "\n",
    "        if len(df) >= 3:\n",
    "            n_legs = len(df)\n",
    "            for i_ in range(n_legs):\n",
    "                for j_ in range(n_legs):\n",
    "                    if i_ == j_:\n",
    "                        continue\n",
    "\n",
    "                    leg_from = df.iloc[i_].to_dict()\n",
    "                    leg_to   = df.iloc[j_].to_dict()\n",
    "                    fr_rank  = leg_from.get('CC_RANK', 9999)\n",
    "                    to_rank  = leg_to.get('CC_RANK', 9999)\n",
    "                    num_between = ((to_rank - fr_rank) % n_legs) - 1\n",
    "\n",
    "                    if num_between == 1:\n",
    "                        x_rank = ((fr_rank + num_between - 1) % n_legs) + 1\n",
    "                        leg_x  = df.loc[df['CC_RANK'] == x_rank]\n",
    "                        if leg_x.empty:\n",
    "                            continue\n",
    "                        leg_xd = leg_x.iloc[0].to_dict()\n",
    "\n",
    "                        fb, tb = leg_from['AVG_BRG'], leg_to['AVG_BRG']\n",
    "                        if not (isinstance(fb, (int, float)) and isinstance(tb, (int, float))):\n",
    "                            continue\n",
    "\n",
    "                        sp = point_from_bearing(jxy, fb, 6)\n",
    "                        ep = point_from_bearing(jxy, tb, 6)\n",
    "                        line = LineString([sp, ep])\n",
    "\n",
    "                        try:\n",
    "                            line_off = line.parallel_offset(offset, 'right', join_style=2, mitre_limit=2)\n",
    "                        except:\n",
    "                            line_off = line\n",
    "\n",
    "                        cross_gdf.at[row_idx, 'geometry']  = line_off\n",
    "                        cross_gdf.at[row_idx, 'JUNC_ID']   = j_id\n",
    "                        cross_gdf.at[row_idx, 'FRM_RANK']  = fr_rank\n",
    "                        cross_gdf.at[row_idx, 'TO_RANK']   = to_rank\n",
    "                        cross_gdf.at[row_idx, 'CRS_RANK']  = x_rank\n",
    "                        cross_gdf.at[row_idx, 'FRM_LEG']   = str(leg_from['LINKS'])\n",
    "                        cross_gdf.at[row_idx, 'TO_LEG']    = str(leg_to['LINKS'])\n",
    "                        cross_gdf.at[row_idx, 'CRS_LEG']   = str(leg_xd['LINKS'])\n",
    "                        cross_gdf.at[row_idx, 'FRM_STNM']  = leg_from['ST_NAME']\n",
    "                        cross_gdf.at[row_idx, 'TO_STNM']   = leg_to['ST_NAME']\n",
    "                        cross_gdf.at[row_idx, 'CRS_STNM']  = leg_xd['ST_NAME']\n",
    "                        row_idx += 1\n",
    "\n",
    "    cross_gdf.crs = roads_geo_df.crs\n",
    "    cross_gdf.to_file(out_path)\n",
    "    print(f\"Saved {len(cross_gdf)} crossings to base directory.\")\n",
    "\n",
    "\n",
    "create_crossings(legs_dict, roads_gdf, crossings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WAS_MERGED\n",
       "False    11720\n",
       "True       632\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junctions_gdf['WAS_MERGED'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
