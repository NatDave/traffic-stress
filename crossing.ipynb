{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded roads_gdf with CRS: EPSG:6491\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Load the necessary libraries\n",
    "###############################\n",
    "\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.ops import unary_union\n",
    "from collections import defaultdict\n",
    "\n",
    "############################################\n",
    "# Set Directory Paths and File Name Strings\n",
    "############################################\n",
    "\n",
    "base_dir = r\"C:\\Users\\natda\\Desktop\\NatDave\\Academics\\PhD_NU\\RESEARCH\\Traffic_Stress\\Boston\"\n",
    "roads_filename = \"boston_streets.shp\"\n",
    "junctions_filename = \"junctions.shp\"\n",
    "crossings_filename = \"crossings.shp\"\n",
    "\n",
    "roads_path = f\"{base_dir}\\\\{roads_filename}\"\n",
    "junctions_path = f\"{base_dir}\\\\{junctions_filename}\"\n",
    "crossings_path = f\"{base_dir}\\\\{crossings_filename}\"\n",
    "\n",
    "#############################\n",
    "# Load the Road Network Data\n",
    "#############################\n",
    "\n",
    "roads_gdf = gpd.read_file(roads_path)\n",
    "if 'unique_id' in roads_gdf.columns and roads_gdf.index.name != 'unique_id':\n",
    "    roads_gdf = roads_gdf.set_index('unique_id', drop=False)\n",
    "\n",
    "print(\"Loaded roads_gdf with CRS:\", roads_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersections saved: 12525 total.\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Create Intersections\n",
    "#######################\n",
    "\n",
    "roads_dict = {}\n",
    "for idx, row in roads_gdf.iterrows():\n",
    "    roads_dict[idx] = {\n",
    "        'qExclude': row.get('qExclude', 0),\n",
    "        'qNoAccess': row.get('qNoAccess', 0),\n",
    "        'geometry': row.geometry,\n",
    "        'StOperNEU': row.get('StOperNEU', 0),\n",
    "        'STREETNAME': row.get('STREETNAME', \"\")\n",
    "    }\n",
    "\n",
    "endpoints_dict = defaultdict(list)\n",
    "for sid, data in roads_dict.items():\n",
    "    qExclude, qNoAccess = data['qExclude'], data['qNoAccess']\n",
    "    geom = data['geometry']\n",
    "    if qExclude == 1 or qNoAccess == 1 or geom is None:\n",
    "        continue\n",
    "\n",
    "    # Handle geometry to get start/end coords\n",
    "    if geom.geom_type == \"LineString\":\n",
    "        coords = geom.coords\n",
    "    elif geom.geom_type == \"MultiLineString\" and len(geom.geoms) > 0:\n",
    "        coords = geom.geoms[0].coords\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        continue\n",
    "\n",
    "    endpoints_dict[coords[0]].append(sid)\n",
    "    endpoints_dict[coords[-1]].append(sid)\n",
    "\n",
    "# Identify potential intersections\n",
    "junctions_list = []\n",
    "junction_id = 1\n",
    "\n",
    "for pt, seg_ids in endpoints_dict.items():\n",
    "    if len(seg_ids) >= 3:\n",
    "        seg_ids_filtered = []\n",
    "        valid = True\n",
    "        for sid in seg_ids:\n",
    "            rd = roads_dict[sid]\n",
    "            if rd['qExclude'] == 1 or rd['qNoAccess'] == 1:\n",
    "                valid = False\n",
    "                break\n",
    "            seg_ids_filtered.append(sid)\n",
    "        if valid:\n",
    "            junctions_list.append({\n",
    "                'JUNC_ID': junction_id,\n",
    "                'INC_LINKS': seg_ids_filtered,\n",
    "                'NUM_LINKS': len(seg_ids_filtered),\n",
    "                'geometry': Point(pt)\n",
    "            })\n",
    "            junction_id += 1\n",
    "\n",
    "def merge_close_nodes_and_add_standalones(junctions, roads, threshold):\n",
    "    \"\"\"\n",
    "    Merge nodes within 'threshold' distance if they're on a divided highway.\n",
    "    Exclude nodes in prevent_merge_ids from being part of any merging cluster.\n",
    "    Otherwise, keep them as separate intersections.\n",
    "    \"\"\"\n",
    "    merged_junctions = []\n",
    "    standalone_junctions = []\n",
    "    processed = set()\n",
    "\n",
    "    # Define IDs to prevent from merging\n",
    "    prevent_merge_ids = {7718, 10131, 4837, 9055}\n",
    "\n",
    "    for idx, junc in junctions.iterrows():\n",
    "        # Skip processing if node ID is in prevention list\n",
    "        if junc['JUNC_ID'] in prevent_merge_ids:\n",
    "            standalone_junctions.append({\n",
    "                'INTER_ID': junc['JUNC_ID'],\n",
    "                'INC_LINKS': junc['INC_LINKS'],\n",
    "                'NUM_LINKS': junc['NUM_LINKS'],\n",
    "                'geometry': junc.geometry,\n",
    "                'WAS_MERGED': False,\n",
    "                'CON_NODES': [junc['JUNC_ID']]  # Keep as standalone\n",
    "            })\n",
    "            processed.add(idx)\n",
    "            continue\n",
    "\n",
    "        if idx in processed:\n",
    "            continue\n",
    "\n",
    "        # Find nodes within threshold, excluding those in prevention list\n",
    "        close_nodes = junctions[\n",
    "            (junctions.geometry.distance(junc.geometry) <= threshold) & \n",
    "            (~junctions['JUNC_ID'].isin(prevent_merge_ids))\n",
    "        ]\n",
    "        inc_links = []\n",
    "        valid_merge = False\n",
    "        constituent_nodes = [junc['JUNC_ID']]  # Start with the current node\n",
    "\n",
    "        for cn in close_nodes.itertuples():\n",
    "            for sid in cn.INC_LINKS:\n",
    "                rd = roads[sid]\n",
    "                geom = rd['geometry']\n",
    "                if rd['StOperNEU'] == 11:\n",
    "                    valid_merge = True\n",
    "                if geom.geom_type == \"LineString\" and geom.length > threshold:\n",
    "                    inc_links.append(sid)\n",
    "                elif geom.geom_type == \"MultiLineString\":\n",
    "                    if any(line.length > threshold for line in geom.geoms):\n",
    "                        inc_links.append(sid)\n",
    "            constituent_nodes.append(cn.JUNC_ID)\n",
    "\n",
    "        # Check if merging will result in at least three legs\n",
    "        if valid_merge and len(close_nodes) > 1 and len(set(inc_links)) >= 3:\n",
    "            mg = unary_union(close_nodes.geometry)\n",
    "            merged_junctions.append({\n",
    "                'INTER_ID': junc['JUNC_ID'],\n",
    "                'INC_LINKS': list(set(inc_links)),\n",
    "                'NUM_LINKS': len(set(inc_links)),\n",
    "                'geometry': mg.centroid,\n",
    "                'WAS_MERGED': True,\n",
    "                'CON_NODES': constituent_nodes  # Add all nodes involved in the merge\n",
    "            })\n",
    "            processed.update(close_nodes.index)\n",
    "        else:\n",
    "            standalone_junctions.append({\n",
    "                'INTER_ID': junc['JUNC_ID'],\n",
    "                'INC_LINKS': junc['INC_LINKS'],\n",
    "                'NUM_LEGS': len(junc['INC_LINKS']),\n",
    "                'geometry': junc.geometry,\n",
    "                'WAS_MERGED': False,\n",
    "                'CON_NODES': [junc['JUNC_ID']]  # Keep as standalone\n",
    "            })\n",
    "            processed.add(idx)\n",
    "\n",
    "    mgdf = gpd.GeoDataFrame(merged_junctions, geometry='geometry', crs=junctions.crs)\n",
    "    sgdf = gpd.GeoDataFrame(standalone_junctions, geometry='geometry', crs=junctions.crs)\n",
    "    return pd.concat([mgdf, sgdf], ignore_index=True)\n",
    "\n",
    "junctions_gdf = gpd.GeoDataFrame(junctions_list, crs=roads_gdf.crs)\n",
    "junctions_gdf = merge_close_nodes_and_add_standalones(junctions_gdf, roads_dict, threshold=27)\n",
    "junctions_gdf.to_file(junctions_path)\n",
    "print(f\"Intersections saved: {len(junctions_gdf)} total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legs_dict created successfully.\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "# Create Legs Dictionary\n",
    "#########################\n",
    "\n",
    "def calc_bearing(jxy, geom):\n",
    "    \"\"\"Compute bearing (0 to 360Â°, north=0) from intersection jxy outward.\"\"\"\n",
    "    if geom.geom_type == \"LineString\":\n",
    "        coords = geom.coords\n",
    "    elif geom.geom_type == \"MultiLineString\" and len(geom.geoms) > 0:\n",
    "        coords = geom.geoms[0].coords\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        return None\n",
    "\n",
    "    sx, sy = coords[0]\n",
    "    ex, ey = coords[-1]\n",
    "\n",
    "    ds = (sx - jxy[0])**2 + (sy - jxy[1])**2\n",
    "    de = (ex - jxy[0])**2 + (ey - jxy[1])**2\n",
    "\n",
    "    if ds < de:\n",
    "        dx, dy = ex - jxy[0], ey - jxy[1]\n",
    "    else:\n",
    "        dx, dy = sx - jxy[0], sy - jxy[1]\n",
    "\n",
    "    angle_rad = math.atan2(dx, dy)\n",
    "    deg = math.degrees(angle_rad)\n",
    "    return deg + 360 if deg < 0 else deg\n",
    "\n",
    "def point_from_bearing(jxy, bearing_deg, dist_m=6):\n",
    "    \"\"\"Compute a point dist_m away from jxy at bearing_deg (0=North, clockwise).\"\"\"\n",
    "    theta = math.radians(bearing_deg)\n",
    "    return (\n",
    "        jxy[0] + dist_m * math.sin(theta),\n",
    "        jxy[1] + dist_m * math.cos(theta)\n",
    "    )\n",
    "\n",
    "legs_dict = {}\n",
    "for _, row in junctions_gdf.iterrows():\n",
    "    j_id = row['INTER_ID']\n",
    "    jxy = (row.geometry.x, row.geometry.y)\n",
    "    INC_LINKS = row['INC_LINKS']\n",
    "\n",
    "    # 1) Build raw_legs\n",
    "    raw_legs = []\n",
    "    for sid in INC_LINKS:\n",
    "        rd = roads_dict[sid]\n",
    "        geom    = rd['geometry']\n",
    "        st_name = rd['STREETNAME']\n",
    "        divided = (rd['StOperNEU'] == 11)\n",
    "\n",
    "        brg = calc_bearing(jxy, geom)\n",
    "        if brg is not None:\n",
    "            raw_legs.append({\n",
    "                'LINKS': [sid],\n",
    "                'ST_NAME': st_name,\n",
    "                'AVG_BRG': brg,\n",
    "                'DIVIDED': divided\n",
    "            })\n",
    "\n",
    "    # 2) Sort by bearing if not empty\n",
    "    df = pd.DataFrame(raw_legs)\n",
    "    if not df.empty and 'AVG_BRG' in df.columns:\n",
    "        df = df.sort_values('AVG_BRG', ascending=False).reset_index(drop=True)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['LINKS','ST_NAME','AVG_BRG','DIVIDED'])\n",
    "\n",
    "    # 3) Merge adjacent divided links if n>3\n",
    "    combined_legs = []\n",
    "    used = set()\n",
    "    n = len(df)\n",
    "\n",
    "    if n > 3:\n",
    "        for i in range(n):\n",
    "            if i in used:\n",
    "                continue\n",
    "            leg_i = df.iloc[i].to_dict()\n",
    "            j = (i + 1) % n\n",
    "            if j not in used and n > 1:\n",
    "                leg_j = df.iloc[j].to_dict()\n",
    "                if leg_i['DIVIDED'] and leg_j['DIVIDED'] and leg_i['ST_NAME'] == leg_j['ST_NAME']:\n",
    "                    combined_legs.append({\n",
    "                        'LINKS': leg_i['LINKS'] + leg_j['LINKS'],\n",
    "                        'ST_NAME': leg_i['ST_NAME'],\n",
    "                        'AVG_BRG': (leg_i['AVG_BRG'] + leg_j['AVG_BRG']) / 2.0,\n",
    "                        'DIVIDED': True\n",
    "                    })\n",
    "                    used.update([i, j])\n",
    "                    continue\n",
    "            combined_legs.append(leg_i)\n",
    "            used.add(i)\n",
    "    else:\n",
    "        combined_legs = [df.iloc[k].to_dict() for k in range(n)]\n",
    "\n",
    "    # 4) Re-sort final & assign rank\n",
    "    if combined_legs:\n",
    "        final_df = pd.DataFrame(combined_legs)\n",
    "        if not final_df.empty and 'AVG_BRG' in final_df.columns:\n",
    "            final_df = final_df.sort_values('AVG_BRG', ascending=False).reset_index(drop=True)\n",
    "            final_df['CC_RANK'] = final_df.index + 1\n",
    "        else:\n",
    "            final_df = pd.DataFrame(columns=['LINKS','ST_NAME','AVG_BRG','DIVIDED','CC_RANK'])\n",
    "    else:\n",
    "        final_df = pd.DataFrame(columns=['LINKS','ST_NAME','AVG_BRG','DIVIDED','CC_RANK'])\n",
    "\n",
    "    legs_dict[j_id] = {\n",
    "        'INTERSECTION_XY': jxy,\n",
    "        'J_NODES': [j_id],\n",
    "        'LEGS_DF': final_df\n",
    "    }\n",
    "\n",
    "print(\"legs_dict created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 41240 crossings to base directory.\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Create Crossing Geometries\n",
    "#############################\n",
    "\n",
    "def create_crossings(legs_dictionary, roads_geo_df, out_path):\n",
    "    cross_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries())\n",
    "    cross_gdf['geometry'] = None\n",
    "    offset = 5\n",
    "    row_idx = 0\n",
    "\n",
    "    for j_id, j_data in legs_dictionary.items():\n",
    "        df = j_data['LEGS_DF']\n",
    "        jxy = j_data['INTERSECTION_XY']\n",
    "\n",
    "        if len(df) >= 3:\n",
    "            n_legs = len(df)\n",
    "            for i_ in range(n_legs):\n",
    "                for j_ in range(n_legs):\n",
    "                    if i_ == j_:\n",
    "                        continue\n",
    "\n",
    "                    leg_from = df.iloc[i_].to_dict()\n",
    "                    leg_to   = df.iloc[j_].to_dict()\n",
    "                    fr_rank  = leg_from.get('CC_RANK', 9999)\n",
    "                    to_rank  = leg_to.get('CC_RANK', 9999)\n",
    "                    num_between = ((to_rank - fr_rank) % n_legs) - 1\n",
    "\n",
    "                    if num_between == 1:\n",
    "                        x_rank = ((fr_rank + num_between - 1) % n_legs) + 1\n",
    "                        leg_x  = df.loc[df['CC_RANK'] == x_rank]\n",
    "                        if leg_x.empty:\n",
    "                            continue\n",
    "                        leg_xd = leg_x.iloc[0].to_dict()\n",
    "\n",
    "                        fb, tb = leg_from['AVG_BRG'], leg_to['AVG_BRG']\n",
    "                        if not (isinstance(fb, (int, float)) and isinstance(tb, (int, float))):\n",
    "                            continue\n",
    "\n",
    "                        sp = point_from_bearing(jxy, fb, 6)\n",
    "                        ep = point_from_bearing(jxy, tb, 6)\n",
    "                        line = LineString([sp, ep])\n",
    "\n",
    "                        try:\n",
    "                            line_off = line.parallel_offset(offset, 'right', join_style=2, mitre_limit=2)\n",
    "                        except:\n",
    "                            line_off = line\n",
    "\n",
    "                        cross_gdf.at[row_idx, 'geometry']  = line_off\n",
    "                        cross_gdf.at[row_idx, 'JUNC_ID']   = j_id\n",
    "                        cross_gdf.at[row_idx, 'FRM_RANK']  = fr_rank\n",
    "                        cross_gdf.at[row_idx, 'TO_RANK']   = to_rank\n",
    "                        cross_gdf.at[row_idx, 'CRS_RANK']  = x_rank\n",
    "                        cross_gdf.at[row_idx, 'FRM_LEG']   = str(leg_from['LINKS'])\n",
    "                        cross_gdf.at[row_idx, 'TO_LEG']    = str(leg_to['LINKS'])\n",
    "                        cross_gdf.at[row_idx, 'CRS_LEG']   = str(leg_xd['LINKS'])\n",
    "                        cross_gdf.at[row_idx, 'FRM_STNM']  = leg_from['ST_NAME']\n",
    "                        cross_gdf.at[row_idx, 'TO_STNM']   = leg_to['ST_NAME']\n",
    "                        cross_gdf.at[row_idx, 'CRS_STNM']  = leg_xd['ST_NAME']\n",
    "                        row_idx += 1\n",
    "\n",
    "    cross_gdf.crs = roads_geo_df.crs\n",
    "    cross_gdf.to_file(out_path)\n",
    "    print(f\"Saved {len(cross_gdf)} crossings to base directory.\")\n",
    "\n",
    "#########################\n",
    "# Run Crossing Creation\n",
    "#########################\n",
    "\n",
    "create_crossings(legs_dict, roads_gdf, crossings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force-merged junctions saved: 12513 total.\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Force Merge Nodes\n",
    "####################\n",
    "\n",
    "def apply_force_merge_only(junctions_gdf, roads_dict, force_merge_ids):\n",
    "    \"\"\"\n",
    "    Apply force merging for specified node pairs and update junctions_gdf.\n",
    "    This function is specifically designed for forced merges without enforcing distance thresholds.\n",
    "    \"\"\"\n",
    "    merged_junctions = []\n",
    "    processed_ids = set()\n",
    "\n",
    "    # Iterate through force_merge_ids to perform merging\n",
    "    for primary_id, secondary_ids in force_merge_ids.items():\n",
    "        # Check if the primary node and secondary nodes are in the dataframe\n",
    "        primary_node = junctions_gdf[junctions_gdf['INTER_ID'] == primary_id]\n",
    "        secondary_nodes = junctions_gdf[junctions_gdf['INTER_ID'].isin(secondary_ids)]\n",
    "\n",
    "        # Skip if primary or secondary nodes are already processed\n",
    "        if primary_id in processed_ids or any(id in processed_ids for id in secondary_ids):\n",
    "            continue\n",
    "\n",
    "        # Combine all nodes into one group\n",
    "        nodes_to_merge = pd.concat([primary_node, secondary_nodes])\n",
    "\n",
    "        # Merge geometries and links\n",
    "        merged_geometry = unary_union(nodes_to_merge.geometry)\n",
    "        merged_links = list(set(nodes_to_merge['INC_LINKS'].sum()))  # Flatten and deduplicate\n",
    "        constituent_ids = list(nodes_to_merge['INTER_ID'])\n",
    "\n",
    "        # Create merged junction\n",
    "        merged_junctions.append({\n",
    "            'INTER_ID': primary_id,\n",
    "            'INC_LINKS': merged_links,\n",
    "            'NUM_LINKS': len(merged_links),\n",
    "            'geometry': merged_geometry.centroid,\n",
    "            'WAS_MERGED': True,\n",
    "            'CON_NODES': constituent_ids\n",
    "        })\n",
    "\n",
    "        # Mark IDs as processed\n",
    "        processed_ids.update(constituent_ids)\n",
    "\n",
    "    # Create a GeoDataFrame for the merged junctions\n",
    "    merged_gdf = gpd.GeoDataFrame(merged_junctions, geometry='geometry', crs=junctions_gdf.crs)\n",
    "\n",
    "    # Remove the force-merged nodes from the original dataframe\n",
    "    remaining_junctions_gdf = junctions_gdf[~junctions_gdf['INTER_ID'].isin(processed_ids)]\n",
    "\n",
    "    # Combine remaining and merged nodes\n",
    "    updated_junctions_gdf = pd.concat([remaining_junctions_gdf, merged_gdf], ignore_index=True)\n",
    "    \n",
    "    return updated_junctions_gdf\\\n",
    "\n",
    "# Define force_merge_ids\n",
    "force_merge_ids = {\n",
    "    868:    [11_486],\n",
    "    867:    [12_172],\n",
    "    387:    [2_773],\n",
    "    87:     [3_784],\n",
    "    4_830:  [4_855],\n",
    "    757:    [8_239],\n",
    "    758:    [871],\n",
    "    6_647:  [11_555],\n",
    "    2_378:  [11_174],\n",
    "    4_977:  [11_952],\n",
    "    4_126:  [8_189],\n",
    "    4_127:  [8_188]\n",
    "}\n",
    "\n",
    "# Apply the forced merging\n",
    "junctions_gdf = apply_force_merge_only(junctions_gdf, roads_dict, force_merge_ids)\n",
    "\n",
    "# Override the shapefile\n",
    "junctions_gdf.to_file(junctions_path)\n",
    "print(f\"Force-merged junctions saved: {len(junctions_gdf)} total.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WAS_MERGED\n",
       "False    11750\n",
       "True       763\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junctions_gdf['WAS_MERGED'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
